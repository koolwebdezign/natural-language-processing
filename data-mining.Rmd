---
title: "Data Mining - Natural Language Processing"
author: "Jack Welch"
date: "July 26, 2017"
output:
  html_document:
    keep_md: yes
---

```{r setup, include=TRUE, message=FALSE, warning=FALSE}
# Use cache inside R markdown file
knitr::opts_chunk$set(cache = TRUE)

# Load the required libraries
library(tm)
library(stringi)
library(NLP)
library(qdap)
library(RWeka)
library(ggplot2)
library(wordcloud)
```

## Course Dataset - from Johns Hopkins University

The dataset that is required to be used as traininig data for this capstone project is located at the following URL.  

* [https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip)

Since this is a rather large dataset, I have begun by doing a manual download and unzip of this data.  I obtained my dataset on the date of this report.  A copy of my data is located locally on my personal computer in the *data* sub-folder.  Due to file size limitations set by GitHub, I will not be uploading my original data files that I have downloaded from this URL.

## Getting and Cleaning the Data

### Load the Data Into R

Use the readlines() function to read the .txt files into memory inside R.

```{r data, warning=FALSE}
# Load the Twitter data
twitter <- readLines(con <- file("./data/en_US.twitter.txt"), encoding = "UTF-8", skipNul = TRUE)
close(con)

# Load the Blog data
blogs <- readLines(con <- file("./data/en_US.blogs.txt"), encoding = "UTF-8", skipNul = TRUE)
close(con)

# Load the News data
news <- readLines(con <- file("./data/en_US.news.txt"), encoding = "UTF-8", skipNul = TRUE)
close(con)

```

### Simple Visualization for Human Inspection

The R head() function allows us to be able to visualize the first six rows of each of the vector variables: twitter, blogs, and news.

```{r file-head}
# Head of Twitter file
head(twitter)

# Head of Blogs file
head(blogs)

# Head of News file
head(news)

```

### Inspect for Length of Datasets

The length() function is used to give us an idea of how many rows are included in each of these datasets.

```{r file-stats}
# Length of Twitter file
length(twitter)

# Length of Blogs file
length(blogs)

# Length of News file
length(news)

```

### Maximum Size of Each Row

Here we use the max() function in order to inspect for the maximum size of each row within the datasets.

```{r file_i}
# Longest entry in Twitter file
twitter_i <- stri_length(twitter)
max(twitter_i)

# Longest entry in Blogs file
blogs_i <- stri_length(blogs)
max(blogs_i)

# Longest entry in News file
news_i <- stri_length(news)
max(news_i)

```

### Convert Datasets to Smaller Sample Datasets

To improve computational performance, we are going to reduce the size of these datasets to a random sample.  We learned within the inference class of this Data Science Specialization that we can use sample data sets and accurately infer facts about the entire population.  We are now going to reduce the datasets from several millions of lines to random samples of 10,000 lines.

```{r file-samples}
set.seed(42)

# Create Twitter Sample
twitterSample <- sample(twitter, 10000)
length(twitterSample)

# Create Blogs Sample
blogsSample <- sample(blogs, 10000)
length(blogsSample)

# Create News Sample
newsSample <- sample(news, 10000)
length(newsSample)

```

### Clean The Data

We were instructed within the scope of this project that we should take a few steps to clean our datasets.  It is desirable to remove excessive punctuation, whitespace, profanity, numbers and more.  

Before we get started, we are going to join the three sample datasets into a single dataset and convert this to our corpus.

```{r data-corpus}
# Join the samples
mySample <- c(blogsSample, newsSample, twitterSample)

# Create the corpus
corpus <- VCorpus(VectorSource(mySample))

```

Let's now do some cleaning using the **tm package** available in R.

```{r data-cleaning}
# Remove troublesome characters using RegEx
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpus <- tm_map(corpus, toSpace, "/|@|\\|")
corpus <- tm_map(corpus, toSpace, "[^[:graph:]]")

# Remove badwords - badwords file obtained from 
# http://www.cs.cmu.edu/~biglou/resources/bad-words.txt
# Load bad words vector
badwords <- readLines("badwords.txt")
# Remove bad words
corpus <- tm_map(corpus, removeWords, badwords)

# Remove punctuation
corpus <- tm_map(corpus, removePunctuation)
# Remove numbers
corpus <- tm_map(corpus, removeNumbers)
# Remove whitespace
corpus <- tm_map(corpus, stripWhitespace)
# Remove stopwords
corpus <- tm_map(corpus, removeWords, stopwords("english"))
# Convert to lower case
corpus <- tm_map(corpus, content_transformer(tolower))

```


## Data Exploration

The data has been cleaned and resides in a corpus which now needs to be transformed into a  document term matrix so that we can apply other tools. I drew upon the work of [Chiara Mondino]{http://54.225.166.221/ChiaraMo/CapstoneMilestone1} for the preparation of my data exploration techniques.  I really liked the sorted visualization demonstrating the most popular words and word combinations.

### Document Term Matrix

```{r dtm}
# Create DocumentTermMatrix
dtm <- DocumentTermMatrix(corpus)
# Output the most frequest terms 
findFreqTerms(dtm, lowfreq = 500)
```

### NGram Analysis


```{R clean-text}
#Prepare data frame from corpus for NGram Tokenization
df <- data.frame(text=unlist(sapply(corpus, `[`, "content")), stringsAsFactors=F)

```


```{r unigram}
# Plot most frequent words (uni-gram)
uniGram <- NGramTokenizer(df, Weka_control(min = 1, max = 1))
# Convert into data frame
one <- data.frame(table(uniGram))
onesorted <- one[order(one$Freq,decreasing = TRUE),]
one20 <- onesorted[1:20,]
colnames(one20) <- c("Word","Frequency")

ggplot(one20, aes(x = reorder(Word, -Frequency), y = Frequency)) + geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + geom_text(aes(label=Frequency), size=3, vjust=-0.2)

```

```{r bigram}
# Plot most frequent couple of words (bi-grams)
biGram <- NGramTokenizer(df, Weka_control(min = 2, max = 2, delimiters = " \\r\\n\\t.,;:\"()?!"))
# Convert into data frame
two <- data.frame(table(biGram))
twosorted <- two[order(two$Freq,decreasing = TRUE),]
two20 <- twosorted[1:20,]
colnames(two20) <- c("Word","Frequency")

ggplot(two20, aes(x = reorder(Word, -Frequency), y = Frequency)) + geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + geom_text(aes(label=Frequency), size=3, vjust=-0.2)

```

```{r trigram}
# Plot most frequent sets of three words (tri-grams)
triGram <- NGramTokenizer(df, Weka_control(min = 3, max = 3, delimiters = " \\r\\n\\t.,;:\"()?!"))
tri <- data.frame(table(triGram))
trisorted <- tri[order(tri$Freq,decreasing = TRUE),]
tri20 <- trisorted[1:20,]
colnames(tri20) <- c("Word","Frequency")

ggplot(tri20, aes(x = reorder(Word, -Frequency), y = Frequency)) + geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + geom_text(aes(label=Frequency), size=3, vjust=-0.2)
```

```{r wordcloud}
wordcloud(tri20$Word, tri20$Freq, max.words=60)
```

## Conclusion & Shiny App Plan

I have inspected here with this initial data exploration exercise that this approach to cleaning my data has removed punctuation from common compound words within the English language like *don't* and *she's*.  What has occurred is the appearance that there are frequently occuring words 's' and 't'.  This will have to be fixed as I move on with this project.  For now, I want this outcome to be documented within this initial report.

It is my intention, moving forward, to produce a Shiny App in accordance with the course objectives.  This Shiny App will be a dynamic web page which will accept user input of a word, or a series of words, and provide a prediction of the next word in order to minimize typing by the user. This has become a common user experience with typing on cell phones and it is our intention to try to mimic that experience with the n-gram predictions shown above.
