---
title: "Data Mining - Natural Language Processing"
author: "Jack Welch"
date: "July 26, 2017"
output:
  html_document:
    keep_md: yes
---

```{r setup, include=TRUE, message=FALSE, warning=FALSE}
# Use cache inside R markdown file
knitr::opts_chunk$set(cache = TRUE)

# Load the tm library
library(tm)

# Load the stringi library
library(stringi)

# Load the NLP library
library(NLP)

```

## Course Dataset - from Johns Hopkins University

The dataset that is required to be used as traininig data for this capstone project is located at the following URL.  

* [https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip)

Since this is a rather large dataset, I have begun by doing a manual download and unzip of this data.  I obtained my dataset on the date of this report.  A copy of my data is located locally on my personal computer in the *data* sub-folder.  Due to file size limitations set by GitHub, I will not be uploading my original data files that I have downloaded from this URL.

## Getting and Cleaning the Data

### Load the Data Into R

Use the readlines() function to read the .txt files into memory inside R.

```{r data, warning=FALSE}
# Load the Twitter data
twitter <- readLines(con <- file("./data/en_US.twitter.txt"), encoding = "UTF-8", skipNul = TRUE)
close(con)

# Load the Blog data
blogs <- readLines(con <- file("./data/en_US.blogs.txt"), encoding = "UTF-8", skipNul = TRUE)
close(con)

# Load the News data
news <- readLines(con <- file("./data/en_US.news.txt"), encoding = "UTF-8", skipNul = TRUE)
close(con)

```

### Simple Visualization for Human Inspection

The R head() function allows us to be able to visualize the first six rows of each of the vector variables: twitter, blogs, and news.

```{r file-head}
# Head of Twitter file
head(twitter)

# Head of Blogs file
head(blogs)

# Head of News file
head(news)

```

### Inspect for Length of Datasets

The length() function is used to give us an idea of how many rows are included in each of these datasets.

```{r file-stats}
# Length of Twitter file
length(twitter)

# Length of Blogs file
length(blogs)

# Length of News file
length(news)

```

### Maximum Size of Each Row

Here we use the max() function in order to inspect for the maximum size of each row within the datasets.

```{r file_i}
# Longest entry in Twitter file
twitter_i <- stri_length(twitter)
max(twitter_i)

# Longest entry in Blogs file
blogs_i <- stri_length(blogs)
max(blogs_i)

# Longest entry in News file
news_i <- stri_length(news)
max(news_i)

```

### Convert Datasets to Smaller Sample Datasets

To improve computational performance, we are going to reduce the size of these datasets to a random sample.  We learned within the inference class of this Data Science Specialization that we can use sample data sets and accurately infer facts about the entire population.  We are now going to reduce the datasets from several millions of lines to random samples of 10,000 lines.

```{r file-samples}
set.seed(42)

# Create Twitter Sample
twitterSample <- sample(twitter, 10000)
length(twitterSample)

# Create Blogs Sample
blogsSample <- sample(blogs, 10000)
length(blogsSample)

# Create News Sample
newsSample <- sample(news, 10000)
length(newsSample)

```

### Clean The Data

We were instructed within the scope of this project that we should take a few steps to clean our datasets.  It is desirable to remove excessive punctuation, whitespace, profanity, numbers and more.  












Before we get started, we are going to join the three sample datasets into a single dataset and call this our corpus.

```{r data-corpus}
# Create the corpus
#corpus <- VCorpus(VectorSource(c(blogsSample, newsSample, twitterSample)))

```

Let's now do some simple cleaning using the **tm package** available in R.

```{r data-cleaning}
# Remove punctuation
#corpus <- tm_map(corpus, removePunctuation)
# Remove numbers
#corpus <- tm_map(corpus, removeNumbers)
# Remove whitespace
#corpus <- tm_map(corpus, stripWhitespace)
# Convert to lower case
#corpus <- tm_map(corpus, content_transformer(tolower))

```